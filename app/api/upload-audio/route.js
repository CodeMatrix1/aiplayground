import { NextRequest, NextResponse } from "next/server";
import { writeFile, mkdir } from "fs/promises";
import { join } from "path";
import { requireAuth } from "../../lib/auth";
import { PrismaClient } from "@prisma/client";
// import { SpeechClient } from "@google-cloud/speech"; // Unused for now
import { GoogleGenerativeAI } from "@google/generative-ai";
import { spawn } from "child_process";
import { existsSync } from "fs";

const prisma = new PrismaClient();
// For Google Speech-to-Text, we need to use a different approach
// We'll use a simple fallback for now since Speech-to-Text requires service account credentials
const speechClient = null; // We'll implement a simpler solution
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);

export async function POST(request) {
  try {
    // Authenticate user
    const user = await requireAuth();

    // Parse form data
    const formData = await request.formData();
    const audioFile = formData.get("audio");

    if (!audioFile) {
      return NextResponse.json(
        { error: "No audio file provided" },
        { status: 400 }
      );
    }

    // Validate audio file
    if (!audioFile.type.startsWith("audio/")) {
      return NextResponse.json(
        { error: "Invalid file type. Please upload an audio file." },
        { status: 400 }
      );
    }

    // Check file size (limit to 25MB)
    const maxSize = 25 * 1024 * 1024; // 25MB
    if (audioFile.size > maxSize) {
      return NextResponse.json(
        { error: "File too large. Please upload an audio file smaller than 25MB." },
        { status: 400 }
      );
    }

    // Create task record
    const task = await prisma.task.create({
      data: {
        userId: user.id,
        type: "CONVERSATION_ANALYSIS",
        status: "PROCESSING",
        input: audioFile.name,
        metadata: {
          fileSize: audioFile.size,
          fileType: audioFile.type,
        },
      },
    });

    try {
      // Create uploads directory if it doesn't exist
      const uploadsDir = join(process.cwd(), "uploads");
      if (!existsSync(uploadsDir)) {
        await mkdir(uploadsDir, { recursive: true });
      }

      // Save file
      const bytes = await audioFile.arrayBuffer();
      const buffer = Buffer.from(bytes);
      const fileName = `${Date.now()}-${audioFile.name}`;
      const filePath = join(uploadsDir, fileName);
      await writeFile(filePath, buffer);

      // Update task with file path
      await prisma.task.update({
        where: { id: task.id },
        data: { input: filePath },
      });

      // Transcribe with Google Speech-to-Text (simplified implementation)
      let transcription;
      try {
        // For now, we'll use a mock transcription since Google Speech-to-Text requires service account setup
        // In production, you would set up proper Google Cloud credentials
        
        console.log("Audio file received:", {
          name: audioFile.name,
          type: audioFile.type,
          size: audioFile.size
        });

        // Mock transcription for demonstration
        // In a real implementation, you would use Google Speech-to-Text API here
        transcription = {
          text: `[Mock transcription] This is a placeholder transcription for the audio file "${audioFile.name}". The actual transcription would be generated by Google Speech-to-Text API. File size: ${(audioFile.size / 1024 / 1024).toFixed(2)}MB, Type: ${audioFile.type}`,
          duration: 30, // Mock duration
          language: 'en-US',
          segments: [
            {
              start: 0,
              end: 30,
              text: "Mock transcription segment"
            }
          ]
        };

        console.log("Mock transcription generated:", {
          text: transcription.text?.substring(0, 100) + "...",
          duration: transcription.duration,
          language: transcription.language
        });
      } catch (transcriptionError) {
        console.error("Transcription failed:", transcriptionError.message);
        throw new Error(`Transcription failed: ${transcriptionError.message}`);
      }

      // Perform speaker diarization using pyannote.audio
      let diarization = [];
      try {
        diarization = await performDiarization(filePath);
      } catch (error) {
        console.error("Diarization failed:", error);
        // Continue without diarization - provide mock data for now
        diarization = [
          {
            speaker: 0,
            start: 0,
            end: transcription.duration || 60,
            text: transcription.text
          }
        ];
      }

      // Generate summary using Gemini
      let summary = "Summary not available";
      try {
        const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
        
        const prompt = `You are a helpful assistant that summarizes conversations. Provide a concise summary of the key points discussed.

Please summarize this conversation:

${transcription.text}`;

        const geminiResult = await model.generateContent(prompt);
        const response = await geminiResult.response;
        summary = response.text();
      } catch (geminiError) {
        console.error("Gemini summary failed:", geminiError.message);
        // Fallback summary
        const wordCount = transcription.text.split(/\s+/).length;
        summary = `This conversation contains ${wordCount} words. The transcription was successful, but summary generation failed due to API issues.`;
      }

      // Format diarization results
      const formattedDiarization = diarization.map(segment => ({
        speaker: segment.speaker,
        start: segment.start,
        end: segment.end,
        text: segment.text || ""
      }));

      const result = {
        transcription: transcription.text,
        diarization: formattedDiarization,
        summary: summary,
        metadata: {
          duration: transcription.duration,
          language: transcription.language,
          segments: transcription.segments?.length || 0,
        }
      };

      // Update task with results
      await prisma.task.update({
        where: { id: task.id },
        data: {
          status: "COMPLETED",
          output: JSON.stringify(result),
          metadata: {
            ...task.metadata,
            duration: transcription.duration,
            language: transcription.language,
            segments: transcription.segments?.length || 0,
            speakers: diarization.length > 0 ? Math.max(...diarization.map(d => d.speaker)) + 1 : 1,
          },
        },
      });

      return NextResponse.json(result);

    } catch (error) {
      // Update task with error
      await prisma.task.update({
        where: { id: task.id },
        data: {
          status: "FAILED",
          output: error.message,
        },
      });

      throw error;
    }

  } catch (error) {
    console.error("Audio processing error:", error);
    console.error("Error details:", {
      message: error.message,
      stack: error.stack,
      name: error.name
    });
    
    if (error.message === "Authentication required") {
      return NextResponse.json(
        { error: "Authentication required" },
        { status: 401 }
      );
    }

    // Return more specific error information
    return NextResponse.json(
      { 
        error: `Failed to process audio file: ${error.message}`,
        details: error.message
      },
      { status: 500 }
    );
  }
}

async function performDiarization(audioPath) {
  return new Promise((resolve, reject) => {
    // Check if the Python script exists
    const scriptPath = join(process.cwd(), 'scripts', 'diarization.py');
    if (!existsSync(scriptPath)) {
      console.log("Diarization script not found, using mock implementation");
      // Return mock diarization data
      resolve([
        {
          speaker: 0,
          start: 0,
          end: 30,
          text: "Mock diarization - speaker detection not available"
        }
      ]);
      return;
    }

    // This would typically call a Python script with pyannote.audio
    // For now, we'll return a mock implementation
    // In production, you'd want to set up a proper diarization service
    
    const pythonProcess = spawn('python', [scriptPath, audioPath]);

    let result = '';
    let error = '';

    pythonProcess.stdout.on('data', (data) => {
      result += data.toString();
    });

    pythonProcess.stderr.on('data', (data) => {
      error += data.toString();
    });

    pythonProcess.on('close', (code) => {
      if (code === 0) {
        try {
          const diarization = JSON.parse(result);
          resolve(diarization);
        } catch (e) {
          reject(new Error('Failed to parse diarization result'));
        }
      } else {
        reject(new Error(`Diarization failed: ${error}`));
      }
    });

    pythonProcess.on('error', (err) => {
      reject(new Error(`Failed to start diarization process: ${err.message}`));
    });
  });
}